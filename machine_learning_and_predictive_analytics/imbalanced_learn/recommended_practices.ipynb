{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Common pitfalls and recommended practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [website](https://imbalanced-learn.org/stable/common_pitfalls.html)\n",
    "\n",
    "## Data Leakage\n",
    "\n",
    "In the resampling setting, there is a common pitfall that corresponds to resample the entire dataset before splitting it into a train and a test partitions. Note that it would be equivalent to resample the train and test partitions as well.\n",
    "\n",
    "Such of a processing leads to two issues:\n",
    "\n",
    "* the model will not be tested on a dataset with class distribution similar to the real use-case. Indeed, by resampling the entire dataset, both the training and testing set will be potentially balanced while the model should be tested on the natural imbalanced dataset to evaluate the potential bias of the model;\n",
    "\n",
    "* the resampling procedure might use information about samples in the dataset to either generate or select some of the samples. Therefore, we might use information of samples which will be later used as testing samples which is the typical data leakage issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will demonstrate the wrong and right ways to do some sampling and emphasize the tools that one should use, avoiding to fall in the trap.\n",
    "\n",
    "We will use the adult census dataset. For the sake of simplicity, we will only use the numerical features. Also, we will make the dataset more imbalanced to increase the effect of the wrongdoings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from imblearn.datasets import make_imbalance\n",
    "\n",
    "X, y = fetch_openml(\n",
    "    data_id=1119, as_frame=True, return_X_y=True\n",
    ")\n",
    "X = X.select_dtypes(include=\"number\")\n",
    "X, y = make_imbalance(\n",
    "    X, y, sampling_strategy={\">50K\": 300}, random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<=50K': 0.988009592326139, '>50K': 0.011990407673860911}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let’s first check the balancing ratio on this dataset:\n",
    "\n",
    "from collections import Counter\n",
    "{key: value / len(y) for key, value in Counter(y).items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To later highlight some of the issue, we will keep aside a left-out set that we will not use for the evaluation of the model:\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, X_left_out, y, y_left_out = train_test_split(\n",
    "    X, y, stratify=y, random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a `sklearn.ensemble.HistGradientBoostingClassifier` as a baseline classifier. First, we will train and check the performance of this classifier, without any preprocessing to alleviate the bias toward the majority class. We evaluate the generalization performance of the classifier via cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy mean +/- std. dev.: 0.609 +/- 0.024\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "model = HistGradientBoostingClassifier(random_state=0)\n",
    "cv_results = cross_validate(\n",
    "    model, X, y, scoring=\"balanced_accuracy\",\n",
    "    return_train_score=True, return_estimator=True,\n",
    "    n_jobs=-1\n",
    ")\n",
    "print(\n",
    "    f\"Balanced accuracy mean +/- std. dev.: \"\n",
    "    f\"{cv_results['test_score'].mean():.3f} +/- \"\n",
    "    f\"{cv_results['test_score'].std():.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the classifier does not give good performance in terms of balanced accuracy mainly due to the class imbalance issue.\n",
    "\n",
    "In the cross-validation, we stored the different classifiers of all folds. We will show that evaluating these classifiers on the left-out data will give close statistical performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy mean +/- std. dev.: 0.628 +/- 0.009\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "scores = []\n",
    "for fold_id, cv_model in enumerate(cv_results[\"estimator\"]):\n",
    "    scores.append(\n",
    "        balanced_accuracy_score(\n",
    "            y_left_out, cv_model.predict(X_left_out)\n",
    "        )\n",
    "    )\n",
    "print(\n",
    "    f\"Balanced accuracy mean +/- std. dev.: \"\n",
    "    f\"{np.mean(scores):.3f} +/- {np.std(scores):.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s now show the wrong pattern to apply when it comes to resampling to alleviate the class imbalance issue. We will use a sampler to balance the entire dataset and check the statistical performance of our classifier via cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy mean +/- std. dev.: 0.724 +/- 0.042\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "sampler = RandomUnderSampler(random_state=0)\n",
    "X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "model = HistGradientBoostingClassifier(random_state=0)\n",
    "cv_results = cross_validate(\n",
    "    model, X_resampled, y_resampled, scoring=\"balanced_accuracy\",\n",
    "    return_train_score=True, return_estimator=True,\n",
    "    n_jobs=-1\n",
    ")\n",
    "print(\n",
    "    f\"Balanced accuracy mean +/- std. dev.: \"\n",
    "    f\"{cv_results['test_score'].mean():.3f} +/- \"\n",
    "    f\"{cv_results['test_score'].std():.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy mean +/- std. dev.: 0.698 +/- 0.014\n"
     ]
    }
   ],
   "source": [
    "# The cross-validation performance looks good, but evaluating the classifiers on the left-out data shows a different picture:\n",
    "\n",
    "scores = []\n",
    "for fold_id, cv_model in enumerate(cv_results[\"estimator\"]):\n",
    "    scores.append(\n",
    "        balanced_accuracy_score(\n",
    "            y_left_out, cv_model.predict(X_left_out)\n",
    "       )\n",
    "    )\n",
    "print(\n",
    "    f\"Balanced accuracy mean +/- std. dev.: \"\n",
    "    f\"{np.mean(scores):.3f} +/- {np.std(scores):.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the performance is now worse than the cross-validated performance. Indeed, the data leakage gave us too optimistic results due to the reason stated earlier in this section.\n",
    "\n",
    "We will now illustrate the correct pattern to use. Indeed, as in scikit-learn, using a `Pipeline` avoids to make any data leakage because the resampling will be delegated to imbalanced-learn and does not require any manual steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy mean +/- std. dev.: 0.732 +/- 0.019\n"
     ]
    }
   ],
   "source": [
    "from imblearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(\n",
    "    RandomUnderSampler(random_state=0),\n",
    "    HistGradientBoostingClassifier(random_state=0)\n",
    ")\n",
    "cv_results = cross_validate(\n",
    "    model, X, y, scoring=\"balanced_accuracy\",\n",
    "    return_train_score=True, return_estimator=True,\n",
    "    n_jobs=-1\n",
    ")\n",
    "print(\n",
    "    f\"Balanced accuracy mean +/- std. dev.: \"\n",
    "    f\"{cv_results['test_score'].mean():.3f} +/- \"\n",
    "    f\"{cv_results['test_score'].std():.3f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced accuracy mean +/- std. dev.: 0.727 +/- 0.008\n"
     ]
    }
   ],
   "source": [
    "# We observe that we get good statistical performance as well, we can check the performance of the model from each cross-validation fold to ensure that we have similar performance:\n",
    "\n",
    "\n",
    "scores = []\n",
    "for fold_id, cv_model in enumerate(cv_results[\"estimator\"]):\n",
    "    scores.append(\n",
    "        balanced_accuracy_score(\n",
    "            y_left_out, cv_model.predict(X_left_out)\n",
    "       )\n",
    "    )\n",
    "print(\n",
    "    f\"Balanced accuracy mean +/- std. dev.: \"\n",
    "    f\"{np.mean(scores):.3f} +/- {np.std(scores):.3f}\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decision_trees_practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
