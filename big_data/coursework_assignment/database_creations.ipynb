{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gathering Data and Database Creations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is to gather data from a kaggle dataset into a sqlite db and api calls to place json into a local mongoDB "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Dataset from kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder 'data' already exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folder_path = \"data\"\n",
    "\n",
    "# creating folder for data to be held, .gitignore data/ added also\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "    print(f\"Folder '{folder_path}' created successfully.\")\n",
    "else:\n",
    "    print(f\"Folder '{folder_path}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set the Kaggle username and key as environment variables for the session\n",
    "# used .env instead json file from kaggle\n",
    "os.environ[\"KAGGLE_USERNAME\"] = os.getenv(\"KAGGLE_USERNAME\")\n",
    "os.environ[\"KAGGLE_KEY\"] = os.getenv(\"KAGGLE_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "# Function to download Kaggle dataset\n",
    "def download_kaggle_dataset(owner, dataset_name, download_path=\"data\"):\n",
    "    os.makedirs(download_path, exist_ok=True)\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "    api.dataset_download_files(f\"{owner}/{dataset_name}\", path=download_path, unzip=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/yakhyojon/tiktok\n"
     ]
    }
   ],
   "source": [
    "# downloading kaggle tiktok dataset\n",
    "download_kaggle_dataset(\"yakhyojon\", \"tiktok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Dataset into a sqlite DB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chatgpt assited in creating db. More tables should be made to be normalized but normalization is not the focus of coursework. I wish to practice interacting with a db."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# loading csv\n",
    "csv_path = 'data/tiktok_dataset.csv'\n",
    "tiktok_data = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating db in data along with connection\n",
    "db_file_path = os.path.join(folder_path, \"tiktok.db\")\n",
    "conn = sqlite3.connect(db_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x7d679570cd50>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cursor = conn.cursor()\n",
    "# Create tables\n",
    "\n",
    "# 1. Videos table\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS Videos (\n",
    "    video_id INTEGER PRIMARY KEY,\n",
    "    video_duration_sec INTEGER,\n",
    "    claim_status TEXT,\n",
    "    verified_status TEXT,\n",
    "    video_transcription_text TEXT\n",
    ")\n",
    "''')\n",
    "\n",
    "# 2. Authors table\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS Authors (\n",
    "    author_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    author_ban_status TEXT\n",
    ")\n",
    "''')\n",
    "\n",
    "# 3. VideoMetrics table\n",
    "cursor.execute('''\n",
    "CREATE TABLE IF NOT EXISTS VideoMetrics (\n",
    "    metric_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    video_id INTEGER,\n",
    "    video_view_count REAL,\n",
    "    video_like_count REAL,\n",
    "    video_share_count REAL,\n",
    "    video_download_count REAL,\n",
    "    video_comment_count REAL,\n",
    "    FOREIGN KEY (video_id) REFERENCES Videos(video_id)\n",
    ")\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Insert data into these tables\n",
    "\n",
    "# Track unique authors and their ban status\n",
    "authors = {}\n",
    "\n",
    "for _, row in tiktok_data.iterrows():\n",
    "    # Insert into Videos table\n",
    "    cursor.execute('''\n",
    "        INSERT OR IGNORE INTO Videos (video_id, video_duration_sec, claim_status, verified_status, video_transcription_text)\n",
    "        VALUES (?, ?, ?, ?, ?)\n",
    "    ''', (row['video_id'], row['video_duration_sec'], row['claim_status'], row['verified_status'], row['video_transcription_text']))\n",
    "\n",
    "    # Insert into Authors table if unique\n",
    "    author_ban_status = row['author_ban_status']\n",
    "    if author_ban_status not in authors:\n",
    "        cursor.execute('''\n",
    "            INSERT INTO Authors (author_ban_status)\n",
    "            VALUES (?)\n",
    "        ''', (author_ban_status,))\n",
    "        authors[author_ban_status] = cursor.lastrowid  # Store the author_id for reference\n",
    "    \n",
    "    # Insert into VideoMetrics table\n",
    "    cursor.execute('''\n",
    "        INSERT INTO VideoMetrics (video_id, video_view_count, video_like_count, video_share_count, video_download_count, video_comment_count)\n",
    "        VALUES (?, ?, ?, ?, ?, ?)\n",
    "    ''', (row['video_id'], row['video_view_count'], row['video_like_count'], row['video_share_count'], row['video_download_count'], row['video_comment_count']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Commit changes\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(7017666017, 59, 'claim', 343296.0, 19425.0), (4014381136, 32, 'claim', 140877.0, 77355.0), (9859838091, 31, 'claim', 902185.0, 97690.0), (1866847991, 25, 'claim', 437506.0, 239954.0), (7105231098, 19, 'claim', 56167.0, 34987.0)]\n"
     ]
    }
   ],
   "source": [
    "# connecting and testing a query\n",
    "conn = sqlite3.connect(db_file_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "query = '''\n",
    "SELECT v.video_id, v.video_duration_sec, v.claim_status, m.video_view_count, m.video_like_count\n",
    "FROM Videos v\n",
    "JOIN VideoMetrics m ON v.video_id = m.video_id\n",
    "LIMIT 5\n",
    "'''\n",
    "result = cursor.execute(query).fetchall()\n",
    "print(result)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mongoDB_practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
